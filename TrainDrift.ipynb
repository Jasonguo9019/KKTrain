{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74a62f-a4a9-4e61-9415-58c8a06f5f75",
   "metadata": {},
   "source": [
    "# Hit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61cb5d-7c43-4a94-9e3c-eb2889e69b88",
   "metadata": {},
   "source": [
    "In this notebook we will try to use Keras and XGBoost in order to distinguish between _true_ conversion electron hits and _fake_ conversion electron hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ce5426-86e5-4f6e-a7d5-a847d6e2ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:32:34.657232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import uproot \n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, ReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b18db5-7d46-40ed-9a1b-528d48256097",
   "metadata": {},
   "source": [
    "First of all we use [uproot](https://uproot.readthedocs.io/en/latest/), a library to reading ROOT files in pure Python and NumPy, to open the `trkana` tree in the `TAKK` folder of the `KKSM01.root` file. We only need certain branches, so we apply a filter to read only `de`, `detsh`, `detshmc`, and `demc`. We then apply a mask to our tree to select only good fits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90450e58-8bce-4fae-be27-97d62d89be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files in /Users/brownd/data/42082053/files.txt\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000005.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000005.root\n",
      " with 580548 hits\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000008.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000008.root\n",
      " with 571683 hits\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000024.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000024.root\n",
      " with 575947 hits\n",
      "Total dataset 1728178 hits\n"
     ]
    }
   ],
   "source": [
    "input_dataset = np.empty\n",
    "temp = np.empty\n",
    "signal = np.empty\n",
    "backgnd = np.empty\n",
    "filelist = os.environ['KKTrainData']\n",
    "print(\"Using files in \" + filelist)\n",
    "files = open(filelist, 'r')\n",
    "for filename in files:\n",
    "    print(\"Processing: \" + filename)    \n",
    "    with uproot.open(filename) as file:\n",
    "        trkana = file[\"TAKK\"][\"trkana\"].arrays(filter_name=\"/de|detsh|detshmc|demc/i\")\n",
    "        trkana = trkana[(trkana['de.goodfit']==1)&(trkana['de.status']>0)&(trkana['demc.proc']==167)]\n",
    "    udt = ak.concatenate(trkana['detsh.udt']).to_numpy()\n",
    "    udoca = ak.concatenate(trkana['detsh.udoca']).to_numpy()\n",
    "    rdrift = ak.concatenate(trkana['detsh.cdrift']).to_numpy()\n",
    "    tottdrift = ak.concatenate(trkana['detsh.tottdrift']).to_numpy()\n",
    "    edep = ak.concatenate(trkana['detsh.edep']).to_numpy()\n",
    "    udocavar = ak.concatenate(trkana['detsh.udocavar']).to_numpy()\n",
    "    hstate = ak.concatenate(trkana['detsh.state']).to_numpy()\n",
    "    print(\"Processed file \" + filename + \" with %s hits\"%udt.shape[0])\n",
    "    temp = np.vstack((udt,udoca,tottdrift,rdrift,edep,udocavar)).T\n",
    "    if input_dataset is np.empty:\n",
    "        input_dataset = temp\n",
    "    else:\n",
    "        input_dataset = np.concatenate((input_dataset, temp))\n",
    "    mcrel = []\n",
    "    mcambig = []\n",
    "    mcdist = []\n",
    "    for i, this_dt in enumerate(trkana['detsh.udt']):\n",
    "        mcrel.extend(trkana['detshmc.rel._rel'][i][:len(this_dt)])\n",
    "        mcambig.extend(trkana['detshmc.ambig'][i][:len(this_dt)])\n",
    "        mcdist.extend(trkana['detshmc.dist'][i][:len(this_dt)])\n",
    "    mcrel = np.array(mcrel)\n",
    "    mcambig = np.array(mcambig)\n",
    "    mcdist = np.array(mcdist)\n",
    "\n",
    "    sig = (hstate>-2) & (mcrel==0) & (mcambig*udoca>0) & (rdrift-mcdist<0.4)\n",
    "    bkg = (hstate>-2) & (mcrel==0) & ((mcambig*udoca<0) | (rdrift-mcdist>0.4))\n",
    "    if signal is np.empty:\n",
    "        signal = sig\n",
    "        backgnd = bkg\n",
    "    else:\n",
    "        signal = np.concatenate((signal,sig))\n",
    "        backgnd = np.concatenate((backgnd,bkg))    \n",
    "nhits=len(input_dataset)\n",
    "print(\"Total dataset %s hits\"%nhits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167fc47-dd84-4a31-98d0-a0270f04bfd5",
   "metadata": {},
   "source": [
    "Here we then assign a label to each hit as _signal_ or _background_, depending on the Monte Carlo truth information. Since the dimension of  `detshmc.rel._rel` is not guaranteed to be the same as the dimension of `detsh` we need to loop over all the entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e8445-92ff-416f-a9a8-2d071b1f0bb9",
   "metadata": {},
   "source": [
    "In our problem we have a different number of signal and background entries in our input dataset. There are several techniques avaialable for _unbalanced_ datasets. Here we are using the most naive one, which is just using $\\min(N_{sig}, N_{bkg})$ events. Then, we divide our input into the _training_, _validation_, and _test_ datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed7b23e-d3ab-4645-a905-c3d4090eb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 242816 matched hits\n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(input_dataset[signal]), len(input_dataset[backgnd]))\n",
    "bsize=32\n",
    "# I need to double the batch_size when truncating as we divide the sample in half later for training\n",
    "tsize=2*bsize\n",
    "min_len = min_len - min_len%tsize\n",
    "print(\"Training on %s matched hits\"%min_len)\n",
    "signal_dataset = input_dataset[signal][:min_len]\n",
    "bkg_dataset = input_dataset[backgnd][:min_len]\n",
    "\n",
    "balanced_input = np.concatenate((signal_dataset, bkg_dataset))\n",
    "y_balanced_input = np.concatenate((np.ones(signal_dataset.shape[0]), np.zeros(bkg_dataset.shape[0])))\n",
    "\n",
    "n_variables = balanced_input.shape[1]\n",
    "\n",
    "x_ce_train, x_ce_test, y_ce_train, y_ce_test = train_test_split(balanced_input, y_balanced_input, test_size=0.5, random_state=42)\n",
    "x_ce_test, x_ce_valid, y_ce_test, y_ce_valid = train_test_split(x_ce_test, y_ce_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414715a1-8fe6-47d2-b856-76e7f09a30c5",
   "metadata": {},
   "source": [
    "## Create and train a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3968ba-51a2-45c8-9882-c989a3f7d691",
   "metadata": {},
   "source": [
    "Here we create a _multi-layer perceptron_ (MLP) model which consists of 3 fully-connected (or _dense_) layers, each one followed by a _dropout_ layer, which helps to avoid overfitting. The model is trained using the [Adam](https://arxiv.org/abs/1412.6980) optimizer and trained for 50 epochs or until the validation loss doesn't improve for 5 epochs (`early_stop`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3a800-39dd-41ff-bd78-c5156fd4919c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:34:07.654545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.5456 - accuracy: 0.7436 - val_loss: 0.5057 - val_accuracy: 0.7683\n",
      "Epoch 2/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4950 - accuracy: 0.7705 - val_loss: 0.4957 - val_accuracy: 0.7738\n",
      "Epoch 3/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4892 - accuracy: 0.7738 - val_loss: 0.4885 - val_accuracy: 0.7762\n",
      "Epoch 4/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4859 - accuracy: 0.7755 - val_loss: 0.4896 - val_accuracy: 0.7751\n",
      "Epoch 5/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4840 - accuracy: 0.7769 - val_loss: 0.4860 - val_accuracy: 0.7763\n",
      "Epoch 6/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4824 - accuracy: 0.7779 - val_loss: 0.4862 - val_accuracy: 0.7780\n",
      "Epoch 7/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4818 - accuracy: 0.7781 - val_loss: 0.4889 - val_accuracy: 0.7768\n",
      "Epoch 8/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4814 - accuracy: 0.7788 - val_loss: 0.4841 - val_accuracy: 0.7791\n",
      "Epoch 9/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4803 - accuracy: 0.7794 - val_loss: 0.4800 - val_accuracy: 0.7796\n",
      "Epoch 10/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4794 - accuracy: 0.7791 - val_loss: 0.4829 - val_accuracy: 0.7795\n",
      "Epoch 11/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4792 - accuracy: 0.7800 - val_loss: 0.4821 - val_accuracy: 0.7785\n",
      "Epoch 12/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4786 - accuracy: 0.7806 - val_loss: 0.4829 - val_accuracy: 0.7790\n",
      "Epoch 13/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4781 - accuracy: 0.7804 - val_loss: 0.4786 - val_accuracy: 0.7818\n",
      "Epoch 14/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4779 - accuracy: 0.7808 - val_loss: 0.4789 - val_accuracy: 0.7818\n",
      "Epoch 15/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4774 - accuracy: 0.7811 - val_loss: 0.4850 - val_accuracy: 0.7782\n",
      "Epoch 16/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4773 - accuracy: 0.7813 - val_loss: 0.4835 - val_accuracy: 0.7791\n",
      "Epoch 17/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4767 - accuracy: 0.7812 - val_loss: 0.4891 - val_accuracy: 0.7766\n",
      "Epoch 18/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4764 - accuracy: 0.7815 - val_loss: 0.4789 - val_accuracy: 0.7814\n",
      "Epoch 19/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4761 - accuracy: 0.7816 - val_loss: 0.4806 - val_accuracy: 0.7810\n",
      "Epoch 20/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4760 - accuracy: 0.7810 - val_loss: 0.4785 - val_accuracy: 0.7814\n",
      "Epoch 21/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4756 - accuracy: 0.7821 - val_loss: 0.4789 - val_accuracy: 0.7812\n",
      "Epoch 22/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4754 - accuracy: 0.7820 - val_loss: 0.4804 - val_accuracy: 0.7813\n",
      "Epoch 23/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4754 - accuracy: 0.7816 - val_loss: 0.4804 - val_accuracy: 0.7808\n",
      "Epoch 24/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4754 - accuracy: 0.7819 - val_loss: 0.4769 - val_accuracy: 0.7828\n",
      "Epoch 25/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4750 - accuracy: 0.7817 - val_loss: 0.4838 - val_accuracy: 0.7793\n",
      "Epoch 26/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4748 - accuracy: 0.7821 - val_loss: 0.4748 - val_accuracy: 0.7834\n",
      "Epoch 27/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4748 - accuracy: 0.7823 - val_loss: 0.4760 - val_accuracy: 0.7833\n",
      "Epoch 28/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4747 - accuracy: 0.7821 - val_loss: 0.4774 - val_accuracy: 0.7829\n",
      "Epoch 29/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4740 - accuracy: 0.7827 - val_loss: 0.4753 - val_accuracy: 0.7827\n",
      "Epoch 30/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4740 - accuracy: 0.7822 - val_loss: 0.4809 - val_accuracy: 0.7809\n",
      "Epoch 31/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4740 - accuracy: 0.7824 - val_loss: 0.4788 - val_accuracy: 0.7814\n",
      "Epoch 32/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4738 - accuracy: 0.7821 - val_loss: 0.4776 - val_accuracy: 0.7816\n",
      "Epoch 33/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4738 - accuracy: 0.7820 - val_loss: 0.4783 - val_accuracy: 0.7813\n",
      "Epoch 34/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4737 - accuracy: 0.7823 - val_loss: 0.4741 - val_accuracy: 0.7835\n",
      "Epoch 35/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4734 - accuracy: 0.7826 - val_loss: 0.4747 - val_accuracy: 0.7820\n",
      "Epoch 36/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4734 - accuracy: 0.7826 - val_loss: 0.4743 - val_accuracy: 0.7838\n",
      "Epoch 37/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4732 - accuracy: 0.7823 - val_loss: 0.4725 - val_accuracy: 0.7844\n",
      "Epoch 38/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4729 - accuracy: 0.7830 - val_loss: 0.4790 - val_accuracy: 0.7813\n",
      "Epoch 39/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4728 - accuracy: 0.7827 - val_loss: 0.4768 - val_accuracy: 0.7832\n",
      "Epoch 40/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4730 - accuracy: 0.7825 - val_loss: 0.4745 - val_accuracy: 0.7837\n",
      "Epoch 41/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4731 - accuracy: 0.7830 - val_loss: 0.4738 - val_accuracy: 0.7840\n",
      "Epoch 42/200\n",
      "7588/7588 [==============================] - 9s 1ms/step - loss: 0.4726 - accuracy: 0.7831 - val_loss: 0.4760 - val_accuracy: 0.7830\n",
      "Epoch 43/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4727 - accuracy: 0.7832 - val_loss: 0.4764 - val_accuracy: 0.7824\n",
      "Epoch 44/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4728 - accuracy: 0.7830 - val_loss: 0.4741 - val_accuracy: 0.7839\n",
      "Epoch 45/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4727 - accuracy: 0.7829 - val_loss: 0.4737 - val_accuracy: 0.7839\n",
      "Epoch 46/200\n",
      "7588/7588 [==============================] - 8s 1ms/step - loss: 0.4725 - accuracy: 0.7833 - val_loss: 0.4748 - val_accuracy: 0.7838\n",
      "Epoch 47/200\n",
      "6209/7588 [=======================>......] - ETA: 1s - loss: 0.4725 - accuracy: 0.7827"
     ]
    }
   ],
   "source": [
    "lay0=Input(shape=(n_variables,),batch_size=1)\n",
    "lay1=Dense(2*n_variables, activation='relu')(lay0)\n",
    "lay2=Dense(2*n_variables, activation='relu')(lay1)\n",
    "lay3=Dense(2*n_variables, activation='relu')(lay2)\n",
    "lay4=Dense(1,activation='sigmoid')(lay3)\n",
    "output_model=Model(inputs=lay0,outputs=lay4)\n",
    "\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "input=Input(shape=(n_variables,),batch_size=bsize)\n",
    "x=Dense(2*n_variables, activation='relu')(input)\n",
    "x=Dense(2*n_variables, activation='relu')(x)\n",
    "x=Dense(2*n_variables, activation='relu')(x)\n",
    "output=Dense(1,activation='sigmoid')(x)\n",
    "model_ce=Model(inputs=input,outputs=output)\n",
    "model_ce.compile(loss='binary_crossentropy',metrics='accuracy',optimizer=opt)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-5, restore_best_weights=True)\n",
    "history_ce = model_ce.fit(x_ce_train, y_ce_train,\n",
    "                          batch_size=bsize,\n",
    "                          epochs=200,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_ce_valid, y_ce_valid),\n",
    "                          callbacks=[early_stop]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb06e-22e5-4ecb-b2c5-48b591268733",
   "metadata": {},
   "source": [
    "## Create and train a Boosted Decision Tree\n",
    "Here, instead of using a MLP, we use a [_Gradient Boosted Decision Tree_](https://xgboost.readthedocs.io/en/stable/) (BDT) to distinguish between signal (true CE hits) and background (fake CE hits). We use the defualt hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e5894-b4e3-405c-a99b-c76b2f12403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = XGBClassifier()\n",
    "model_xgboost.fit(x_ce_train, y_ce_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa6212-21f7-46c8-b7af-dab07ced4269",
   "metadata": {},
   "source": [
    "Here we can finally apply our two models (the MLP and the BDT) to our test datasets and create the corresponding ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbef70-e875-40dd-8a8f-5a66fc32fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ce = model_ce.predict(x_ce_test).ravel()\n",
    "fpr_ce, tpr_ce, th_ce = roc_curve(y_ce_test,  prediction_ce)\n",
    "auc_ce = roc_auc_score(y_ce_test, prediction_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6a838-1a4c-41e3-a2bb-46a270ad6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xgboost = model_xgboost.predict_proba(x_ce_test)[:,1]\n",
    "fpr_xgboost, tpr_xgboost, th_xgboost = roc_curve(y_ce_test,  prediction_xgboost)\n",
    "auc_xgboost = roc_auc_score(y_ce_test, prediction_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f392a2-223f-4a47-81de-45dc56b8e42d",
   "metadata": {},
   "source": [
    "The plot of the ROC curves clearly shows that the BDT outperforms the MLP. In principle, however, it should be possible to improve the MLP performances by optimizing the hyperparameters (learning rate, hidden layers, activation functions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9b80f-d130-4ac0-bc19-ea03de7032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(tpr_ce,1-fpr_ce,label=f'MLP AUC: {auc_ce:.3f}')\n",
    "ax.plot(tpr_xgboost,1-fpr_xgboost,label=f'BDT AUC: {auc_xgboost:.3f}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Signal efficiency\")\n",
    "ax.set_ylabel(\"Background rejection\")\n",
    "ax.set_xlim(0,1.05)\n",
    "ax.set_ylim(0,1.05)\n",
    "fig.savefig(\"TrainDrift.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebbd1c-7365-43ce-acb3-873ad2f17d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model.set_weights(model_ce.get_weights())\n",
    "output_model.summary()\n",
    "output_model.save(\"TrainDrift.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KKTrain",
   "language": "python",
   "name": "kktrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
