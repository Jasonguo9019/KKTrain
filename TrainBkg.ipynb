{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74a62f-a4a9-4e61-9415-58c8a06f5f75",
   "metadata": {},
   "source": [
    "# Hit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a72177-e0cc-49d1-8c46-242d9955b3f9",
   "metadata": {},
   "source": [
    "Set initial variables for the \"suffix\" identifier for the data set and \"filePath\" the relative path to the data files. Remember to add \"/\" to the end of your path, for example \"../../TrkAna/43291981/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6004c9-6161-479a-a0a5-2bf504632518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# suffix = \"TTtpr\"\n",
    "# treename = \"TAtpr\"\n",
    "# file_list = \"/global/cfs/cdirs/m3712/Mu2e/TrkAna/60358177/files.txt\"\n",
    "# print(\"Using files in \" + file_list)\n",
    "print(\"Loaded TrainBkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61cb5d-7c43-4a94-9e3c-eb2889e69b88",
   "metadata": {},
   "source": [
    "In this notebook we use Keras and XGBoost to distinguish between hits from conversion electrons and hits from other particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce5426-86e5-4f6e-a7d5-a847d6e2ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(file_list):\n",
    "    raise SystemExit(\"File list \" +file_list + \" doesn't exist! Exiting\")\n",
    "import uproot \n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, ReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42fbce-43c1-42c1-a8da-2878e26a4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b18db5-7d46-40ed-9a1b-528d48256097",
   "metadata": {},
   "source": [
    "In our problem we have a different number of signal and background entries in our input dataset. There are several techniques avaialable for _unbalanced_ datasets. Here we are using the most naive one, which is just using $\\min(N_{sig}, N_{bkg})$ events. Then, we divide our input into the _training_, _validation_, and _test_ datasets.  Note that the datasets must be pruned to the nearest multiple of the batch size, otherwise the gradient calculation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90450e58-8bce-4fae-be27-97d62d89be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = np.empty\n",
    "temp = np.empty\n",
    "signal = np.empty\n",
    "backgnd = np.empty\n",
    "files = open(file_list, 'r')\n",
    "for filename in files:\n",
    "    print(\"Processing file \" + filename)\n",
    "    # pathToFile = Path(filepath1 + filename)\n",
    "    # print (filepath1+filename)\n",
    "    # print(pathToFile)\n",
    "    # file = uproot.open(pathToFile)\n",
    "    with uproot.open(filename) as file:\n",
    "        trkana = file[treename][\"trkana\"].arrays(filter_name=\"/de|detsh|detshmc|demc/i\")\n",
    "        trkana = trkana[(trkana['de.goodfit']==1)&(trkana['de.status']>0)&(trkana['demc.proc']==167)]\n",
    "        hstate = ak.concatenate(trkana['detsh.state']).to_numpy()\n",
    "        udoca = ak.concatenate(trkana['detsh.udoca']).to_numpy()\n",
    "        udoca = np.absolute(udoca)\n",
    "        cdrift = ak.concatenate(trkana['detsh.cdrift']).to_numpy()\n",
    "        rdrift = ak.concatenate(trkana['detsh.rdrift']).to_numpy()\n",
    "        tottdrift = ak.concatenate(trkana['detsh.tottdrift']).to_numpy()\n",
    "        edep = ak.concatenate(trkana['detsh.edep']).to_numpy()\n",
    "        sint = ak.concatenate(trkana['detsh.wdot']).to_numpy()\n",
    "        sint = np.sqrt(1.0-sint*sint)\n",
    "        plen = 6.25-rdrift*rdrift\n",
    "        pmin = np.repeat(0.25,plen.shape[0])\n",
    "        plen = np.sqrt(np.maximum(plen,pmin))\n",
    "        edep = edep*sint/plen\n",
    "        udocasig = ak.concatenate(trkana['detsh.udocavar']).to_numpy()\n",
    "        udocasig = np.sqrt(udocasig)\n",
    "        wdist = ak.concatenate(trkana['detsh.wdist']).to_numpy()\n",
    "        uupos = ak.concatenate(trkana['detsh.uupos']).to_numpy()\n",
    "        du = wdist-uupos\n",
    "        du = np.absolute(du)\n",
    "        rho = np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fX']).to_numpy())\n",
    "        rho = np.add(rho,np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fY']).to_numpy()))\n",
    "        rho = np.sqrt(rho)\n",
    "        print(\"Processed file \" + filename + \" with %s hits\"%hstate.shape[0])\n",
    "        temp = np.vstack((udoca, cdrift, udocasig, tottdrift, edep, du, rho)).T\n",
    "        if input_dataset is np.empty:\n",
    "            input_dataset = temp\n",
    "        else:\n",
    "            input_dataset = np.concatenate((input_dataset, temp))\n",
    "        mcrel = []\n",
    "        for i, this_dt in enumerate(trkana['detsh.state']):\n",
    "            mcrel.extend(trkana['detshmc.rel._rel'][i][:len(this_dt)])\n",
    "        mcrel = np.array(mcrel)\n",
    "        rand = np.random.random_sample([mcrel.shape[0]])\n",
    "        sig = (hstate>=-2) & (rand<0.05) & (mcrel==0) & (udoca < 50.0) & (du < 3000.0)\n",
    "        bkg = (hstate>=-2) & (mcrel==-1) & (udoca < 50.0) & ( du < 3000.0)\n",
    "        if signal is np.empty:\n",
    "            signal = sig\n",
    "            backgnd = bkg\n",
    "        else:\n",
    "            signal = np.concatenate((signal,sig))\n",
    "            backgnd = np.concatenate((backgnd,bkg))\n",
    "nhits=len(input_dataset)\n",
    "nsignal=signal.sum()\n",
    "nbackgnd=backgnd.sum()\n",
    "print(\"Total dataset %s hits, %s signal and %s background\"%(nhits,nsignal,nbackgnd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7b23e-d3ab-4645-a905-c3d4090eb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(input_dataset[signal]), len(input_dataset[backgnd]))\n",
    "bsize=32\n",
    "# I need to double the batch_size when truncating as we divide the sample in half later for training\n",
    "tsize=2*bsize\n",
    "min_len = min_len - min_len%tsize\n",
    "print(\"Training on %s matched hits\"%min_len)\n",
    "signal_dataset = input_dataset[signal][:min_len]\n",
    "bkg_dataset = input_dataset[backgnd][:min_len]\n",
    "\n",
    "balanced_input = np.concatenate((signal_dataset, bkg_dataset))\n",
    "y_balanced_input = np.concatenate((np.ones(signal_dataset.shape[0]), np.zeros(bkg_dataset.shape[0])))\n",
    "\n",
    "n_variables = balanced_input.shape[1]\n",
    "\n",
    "x_ce_train, x_ce_test, y_ce_train, y_ce_test = train_test_split(balanced_input, y_balanced_input, test_size=0.5, random_state=42)\n",
    "x_ce_test, x_ce_valid, y_ce_test, y_ce_valid = train_test_split(x_ce_test, y_ce_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7bdb7-ae06-43a7-a037-f81bb4f3fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "udoca_sig = []\n",
    "tottdrift_sig = []\n",
    "cdrift_sig = []\n",
    "udocasig_sig = []\n",
    "edep_sig = []\n",
    "du_sig = []\n",
    "rho_sig = []\n",
    "\n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    udoca_sig.append(signal_dataset[i][0])\n",
    "    \n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    cdrift_sig.append(signal_dataset[i][1])\n",
    "    \n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    udocasig_sig.append(signal_dataset[i][2])\n",
    "\n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    tottdrift_sig.append(signal_dataset[i][3])\n",
    "    \n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    edep_sig.append(signal_dataset[i][4])\n",
    "\n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    du_sig.append(signal_dataset[i][5])\n",
    "\n",
    "for i in range(signal_dataset.shape[0]):\n",
    "    rho_sig.append(signal_dataset[i][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726da8f6-e232-49f6-aea7-692b5e5134f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "udoca_back = []\n",
    "tottdrift_back = []\n",
    "cdrift_back = []\n",
    "udocasig_back = []\n",
    "edep_back = []\n",
    "du_back = []\n",
    "rho_back = []\n",
    "    \n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    udoca_back.append(bkg_dataset[i][0])\n",
    "    \n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    cdrift_back.append(bkg_dataset[i][1])\n",
    "    \n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    udocasig_back.append(bkg_dataset[i][2])\n",
    "\n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    tottdrift_back.append(bkg_dataset[i][3])\n",
    "\n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    edep_back.append(bkg_dataset[i][4])\n",
    "    \n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    du_back.append(bkg_dataset[i][5])\n",
    "    \n",
    "for i in range(bkg_dataset.shape[0]):\n",
    "    rho_back.append(bkg_dataset[i][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603b3c5-a082-4475-aa77-cfa3f85107ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(udoca_sig,label=\"Unbiased DOCA Signal\", bins=100,range=(0,15))\n",
    "plt.hist(udoca_back,label=\"Unbiased DOCA Background\", histtype='step', bins=100,range=(0,15))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b0283-44ee-43c4-a21e-a5e431bf98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cdrift_sig,label=\"Drift Radius Signal\", bins=50,range=(-3.0,5.0))\n",
    "plt.hist(cdrift_back,label=\"Drift Radius Background\", histtype='step', bins=50, range=(-3.0,5.0))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641193d8-ec0c-4a47-8197-a89a3c7bfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(udocasig_sig,label=\"DOCA Variance Signal\", bins=50,range=(0,5))\n",
    "plt.hist(udocasig_back,label=\"DOCA Variance Background\", histtype='step', bins=50,range=(0,5))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8f1c8-7724-47e4-af28-e533929bab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tottdrift_sig,label=\"Time-Over-Threshold Drift Time Signal\", bins=50, range=(0,40))\n",
    "plt.hist(tottdrift_back,label=\"Time-Over-Threshold Drift Time Background\", histtype='step', bins=50,range=(0,40))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d7b66-d452-4f98-a440-ea92f609a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(edep_sig,label=\"Energy Deposition Signal\",bins=50,range=(0,0.007))\n",
    "plt.hist(edep_back,label=\"Energy Deposition Background\", histtype='step', bins=50, range=(0,0.007))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef6cb0-2aa9-4508-b0b7-0413fe9ea9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(du_sig,label=\"WDist - Unbiased U Position Difference Signal\", bins=50, range=(0,800))\n",
    "plt.hist(du_back,label=\"WDist - Unbiased U Position Difference Background\", histtype='step', bins=50, range=(0,800))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da4306-e9c3-4e7d-ae51-927277b85ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rho_sig,label=\"Rho Signal\", bins=50, range=(350,700))\n",
    "plt.hist(rho_back,label=\"Rho Background\", histtype='step', bins=50, range=(350,700))\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afa96c-4517-4bca-95c5-53ef01a36dc8",
   "metadata": {},
   "source": [
    "Then, we concatenate each hit variables into a single, large numpy array. These arrays are then stacked in a single bi-dimensional array with `np.vstack`, which will be our input dataset used for the training of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167fc47-dd84-4a31-98d0-a0270f04bfd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we then assign a label to each hit as _signal_ or _background_, depending on the Monte Carlo truth information. Since the dimension of  `detshmc.rel._rel` is not guaranteed to be the same as the dimension of `detsh` we need to loop over all the entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414715a1-8fe6-47d2-b856-76e7f09a30c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create and train a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3968ba-51a2-45c8-9882-c989a3f7d691",
   "metadata": {},
   "source": [
    "Here we create a _multi-layer perceptron_ (MLP) model which consists of 3 fully-connected (or _dense_) layers, each one followed by a _dropout_ layer, which helps to avoid overfitting. The model is trained using the [Adam](https://arxiv.org/abs/1412.6980) optimizer and trained for 50 epochs or until the validation loss doesn't improve for 5 epochs (`early_stop`). The model we save must be created first, with an explicit input layer with explicit batch_size, otherwise it can't be parsed by the TMVA::SOFIE parser we use to generate a C++ inference function downstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907660a-6c6a-47cd-a59f-1321824bfb4e",
   "metadata": {},
   "source": [
    "The output model must have batch_size=1, otherwise the SOFIE inference function will assume that many input variables at a time.  Training (gradient calculation) however is much more efficient with a larger batch size, so we construct a separate model for that.  After training and before saving, we'll copy the weights (which don't depend on batch_size) from the trained model to the output model.  This should be unnecessary in the next verison of ROOT.\n",
    "\n",
    "We should initialize the model by reading a previous iteration. TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d67f9b-a0f8-4b94-8051-c9a5001eed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previous training if it exists\n",
    "modelfile = \"models/TrainBkg\" + suffix + \".h5\"\n",
    "if os.path.isfile(modelfile):\n",
    "    model = keras.models.load_model(modelfile)\n",
    "    print(\"Loading model from file \" +modelfile)\n",
    "else:\n",
    "    print(\"Creating model \" +modelfile)\n",
    "    lay0=Input(shape=(n_variables,),batch_size=bsize)\n",
    "    lay1=Dense(2*n_variables, activation='relu')(lay0)\n",
    "    lay2=Dense(2*n_variables, activation='relu')(lay1)\n",
    "    lay3=Dense(2*n_variables, activation='relu')(lay2)\n",
    "    lay4=Dense(1,activation='sigmoid')(lay3)\n",
    "    model=Model(inputs=lay0,outputs=lay4)\n",
    "\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "model.compile(loss='binary_crossentropy',metrics='accuracy',optimizer=opt)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, min_delta=1e-5, restore_best_weights=True)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(x_ce_train, y_ce_train,\n",
    "                          batch_size=bsize,\n",
    "                          epochs=200,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_ce_valid, y_ce_valid),\n",
    "                          callbacks=[early_stop, tensorboard_callback]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ffe4b5-3a13-4fca-b300-ac29d34e58d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'],label=\"val loss\")\n",
    "plt.plot(history.history['loss'],label=\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "%load_ext tensorboard\n",
    "#rm -rf ./logs/\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb06e-22e5-4ecb-b2c5-48b591268733",
   "metadata": {},
   "source": [
    "## Create and train a Boosted Decision Tree\n",
    "Here, instead of using a MLP, we use a [_Gradient Boosted Decision Tree_](https://xgboost.readthedocs.io/en/stable/) (BDT) to distinguish between signal (true CE hits) and background (fake CE hits). We use the defualt hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e5894-b4e3-405c-a99b-c76b2f12403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = XGBClassifier(use_label_encoder=False)\n",
    "model_xgboost.fit(x_ce_train, y_ce_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa6212-21f7-46c8-b7af-dab07ced4269",
   "metadata": {},
   "source": [
    "Here we can finally apply our two models (the MLP and the BDT) to our test datasets and create the corresponding ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbef70-e875-40dd-8a8f-5a66fc32fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction_ce = model.predict(x_ce_test).ravel()\n",
    "prediction_ce = model.predict(x_ce_test)\n",
    "fpr_ce, tpr_ce, th_ce = roc_curve(y_ce_test,  prediction_ce)\n",
    "auc_ce = roc_auc_score(y_ce_test, prediction_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6a838-1a4c-41e3-a2bb-46a270ad6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xgboost = model_xgboost.predict_proba(x_ce_test)[:,1]\n",
    "fpr_xgboost, tpr_xgboost, th_xgboost = roc_curve(y_ce_test,  prediction_xgboost)\n",
    "auc_xgboost = roc_auc_score(y_ce_test, prediction_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f392a2-223f-4a47-81de-45dc56b8e42d",
   "metadata": {},
   "source": [
    "The plot of the ROC curves clearly shows that the BDT outperforms the MLP. In principle, however, it should be possible to improve the MLP performances by optimizing the hyperparameters (learning rate, hidden layers, activation functions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9b80f-d130-4ac0-bc19-ea03de7032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(tpr_ce,1-fpr_ce,label=f'MLP AUC: {auc_ce:.4f}')\n",
    "ax.plot(tpr_xgboost,1-fpr_xgboost,label=f'BDT AUC: {auc_xgboost:.4f}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Signal efficiency\")\n",
    "ax.set_ylabel(\"Background rejection\")\n",
    "ax.set_xlim(0.8,1.05)\n",
    "ax.set_ylim(0.8,1.05)\n",
    "\n",
    "fig.savefig(\"plots/TrainBkg\" + suffix + \".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db6b41-b065-469d-a42d-70a92329ec12",
   "metadata": {},
   "source": [
    "Now we save our model in HDF5 format.  This can be used as input to the SOFIE parser.  Note that the kernel must be restarted when saving this file, as re-running individual cells increments the layer numbers in the hdf5 file, causing the SOFIE parser to fail.  This causes the spurious tensorflow warning about the model not having been built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18723eb7-258e-4687-a58c-cd0afa2c41e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelfile)\n",
    "print(\"model saved to \" +modelfile)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KKTrain",
   "language": "python",
   "name": "kktrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
