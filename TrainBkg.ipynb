{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74a62f-a4a9-4e61-9415-58c8a06f5f75",
   "metadata": {},
   "source": [
    "# Hit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61cb5d-7c43-4a94-9e3c-eb2889e69b88",
   "metadata": {},
   "source": [
    "In this notebook we will try to use Keras and XGBoost in order to distinguish between _true_ conversion electron hits and _fake_ conversion electron hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ce5426-86e5-4f6e-a7d5-a847d6e2ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 14:14:09.172330: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import uproot \n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, ReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b18db5-7d46-40ed-9a1b-528d48256097",
   "metadata": {},
   "source": [
    "First of all we use [uproot](https://uproot.readthedocs.io/en/latest/), a library to reading ROOT files in pure Python and NumPy, to open the `trkana` tree in the `TAKK` folder of the `KKSM01.root` file. We only need certain branches, so we apply a filter to read only `de`, `detsh`, `detshmc`, and `demc`. We then apply a mask to our tree to select only good fits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90450e58-8bce-4fae-be27-97d62d89be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open(\"/Users/brownd/data/KKSNB02.root\") as file:\n",
    "    trkana = file[\"TAKK\"][\"trkana\"].arrays(filter_name=\"/de|detsh|detshmc|demc/i\")\n",
    "    trkana = trkana[(trkana['de.goodfit']==1)&(trkana['de.status']>0)&(trkana['demc.proc']==167)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afa96c-4517-4bca-95c5-53ef01a36dc8",
   "metadata": {},
   "source": [
    "Then, we concatenate each hit variables into a single, large numpy array. These arrays are then stacked in a single bi-dimensional array with `np.vstack`, which will be our input dataset used for the training of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b84b76-ff6a-44f2-88d6-39cbeb3def87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798008\n",
      "(798008, 9)\n"
     ]
    }
   ],
   "source": [
    "udt = ak.concatenate(trkana['detsh.udt']).to_numpy()\n",
    "udoca = ak.concatenate(trkana['detsh.udoca']).to_numpy()\n",
    "rdrift = ak.concatenate(trkana['detsh.rdrift']).to_numpy()\n",
    "tottdrift = ak.concatenate(trkana['detsh.tottdrift']).to_numpy()\n",
    "edep = ak.concatenate(trkana['detsh.edep']).to_numpy()\n",
    "udocavar = ak.concatenate(trkana['detsh.udocavar']).to_numpy()\n",
    "wdist = ak.concatenate(trkana['detsh.wdist']).to_numpy()\n",
    "uupos = ak.concatenate(trkana['detsh.uupos']).to_numpy()\n",
    "rho = np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fX']).to_numpy())\n",
    "rho = np.add(rho,np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fY']).to_numpy()))\n",
    "rho = np.sqrt(rho)\n",
    "n_events = udt.shape[0]\n",
    "print(n_events)\n",
    "\n",
    "input_dataset = np.vstack((udt,udoca,rdrift,tottdrift,edep,udocavar,wdist,uupos,rho)).T\n",
    "print(np.shape(input_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167fc47-dd84-4a31-98d0-a0270f04bfd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we then assign a label to each hit as _signal_ or _background_, depending on the Monte Carlo truth information. Since the dimension of  `detshmc.rel._rel` is not guaranteed to be the same as the dimension of `detsh` we need to loop over all the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e546d312-4d8b-4757-9b0e-7293d9eae321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798008\n"
     ]
    }
   ],
   "source": [
    "mcrel = []\n",
    "for i, this_dt in enumerate(trkana['detsh.udt']):\n",
    "    mcrel.extend(trkana['detshmc.rel._rel'][i][:len(this_dt)])\n",
    "n_mcevents = len(mcrel)\n",
    "print(n_mcevents)\n",
    "    \n",
    "mcrel = np.array(mcrel)\n",
    "signal = mcrel==0\n",
    "bkg = mcrel==-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e8445-92ff-416f-a9a8-2d071b1f0bb9",
   "metadata": {},
   "source": [
    "In our problem we have a different number of signal and background entries in our input dataset. There are several techniques avaialable for _unbalanced_ datasets. Here we are using the most naive one, which is just using $\\min(N_{sig}, N_{bkg})$ events. Then, we divide our input into the _training_, _validation_, and _test_ datasets.  Note that the datasets must be pruned to the nearest multiple of the batch size, otherwise the gradient calculation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed7b23e-d3ab-4645-a905-c3d4090eb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 0\n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(input_dataset[signal]), len(input_dataset[bkg]))\n",
    "bsize=32\n",
    "# I need to double the batch_size when truncating as we divide the sample in half later for training\n",
    "tsize=2*bsize\n",
    "min_len = min_len - min_len%tsize\n",
    "print(min_len, min_len%bsize)\n",
    "signal_dataset = input_dataset[signal][:min_len]\n",
    "bkg_dataset = input_dataset[bkg][:min_len]\n",
    "\n",
    "balanced_input = np.concatenate((signal_dataset, bkg_dataset))\n",
    "y_balanced_input = np.concatenate((np.ones(signal_dataset.shape[0]), np.zeros(bkg_dataset.shape[0])))\n",
    "\n",
    "n_variables = balanced_input.shape[1]\n",
    "\n",
    "x_ce_train, x_ce_test, y_ce_train, y_ce_test = train_test_split(balanced_input, y_balanced_input, test_size=0.5, random_state=42)\n",
    "x_ce_test, x_ce_valid, y_ce_test, y_ce_valid = train_test_split(x_ce_test, y_ce_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414715a1-8fe6-47d2-b856-76e7f09a30c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create and train a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3968ba-51a2-45c8-9882-c989a3f7d691",
   "metadata": {},
   "source": [
    "Here we create a _multi-layer perceptron_ (MLP) model which consists of 3 fully-connected (or _dense_) layers, each one followed by a _dropout_ layer, which helps to avoid overfitting. The model is trained using the [Adam](https://arxiv.org/abs/1412.6980) optimizer and trained for 50 epochs or until the validation loss doesn't improve for 5 epochs (`early_stop`). The model we save must be created first, with an explicit input layer with explicit batch_size, otherwise it can't be parsed by the TMVA::SOFIE parser we use to generate a C++ inference function downstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907660a-6c6a-47cd-a59f-1321824bfb4e",
   "metadata": {},
   "source": [
    "The output model must have batch_size=1, otherwise the SOFIE inference function will assume that many input variables at a time.  Training (gradient calculation) however is much more efficient with a larger batch size, so we construct a separate model for that.  After training and before saving, we'll copy the weights (which don't depend on batch_size) from the trained model to the output model.  This should be unnecessary in the next verison of ROOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d67f9b-a0f8-4b94-8051-c9a5001eed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:EarlyStopping mode <built-in function min> is unknown, fallback to auto mode.\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 14:14:47.897763: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 2ms/step - loss: 3.6305 - accuracy: 0.4894 - val_loss: 0.8350 - val_accuracy: 0.5257\n",
      "Epoch 2/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7386 - accuracy: 0.5401 - val_loss: 0.6964 - val_accuracy: 0.5400\n",
      "Epoch 3/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6799 - accuracy: 0.5694 - val_loss: 0.6815 - val_accuracy: 0.5685\n",
      "Epoch 4/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6566 - accuracy: 0.5890 - val_loss: 0.6283 - val_accuracy: 0.6242\n",
      "Epoch 5/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.6223 - val_loss: 0.6100 - val_accuracy: 0.6520\n",
      "Epoch 6/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.5581 - accuracy: 0.6974 - val_loss: 0.5362 - val_accuracy: 0.7368\n",
      "Epoch 7/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.5384 - accuracy: 0.7487 - val_loss: 0.5264 - val_accuracy: 0.7625\n",
      "Epoch 8/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4716 - accuracy: 0.7968 - val_loss: 0.4534 - val_accuracy: 0.8037\n",
      "Epoch 9/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.8070 - val_loss: 0.4719 - val_accuracy: 0.7755\n",
      "Epoch 10/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8127 - val_loss: 0.4512 - val_accuracy: 0.8045\n",
      "Epoch 11/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8091 - val_loss: 0.4388 - val_accuracy: 0.8125\n",
      "Epoch 12/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4189 - accuracy: 0.8174 - val_loss: 0.4231 - val_accuracy: 0.8173\n",
      "Epoch 13/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8210 - val_loss: 0.4186 - val_accuracy: 0.8207\n",
      "Epoch 14/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.4098 - accuracy: 0.8215 - val_loss: 0.5954 - val_accuracy: 0.7790\n",
      "Epoch 15/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8351 - val_loss: 0.4309 - val_accuracy: 0.8077\n",
      "Epoch 16/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8405 - val_loss: 0.3705 - val_accuracy: 0.8505\n",
      "Epoch 17/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.8537 - val_loss: 0.3516 - val_accuracy: 0.8550\n",
      "Epoch 18/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3195 - accuracy: 0.8731 - val_loss: 0.3302 - val_accuracy: 0.8668\n",
      "Epoch 19/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.8785 - val_loss: 0.2990 - val_accuracy: 0.8848\n",
      "Epoch 20/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2768 - accuracy: 0.8867 - val_loss: 0.2958 - val_accuracy: 0.8850\n",
      "Epoch 21/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2593 - accuracy: 0.8954 - val_loss: 0.2806 - val_accuracy: 0.8925\n",
      "Epoch 22/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9011 - val_loss: 0.2556 - val_accuracy: 0.9087\n",
      "Epoch 23/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9047 - val_loss: 0.2489 - val_accuracy: 0.9120\n",
      "Epoch 24/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9124 - val_loss: 0.2386 - val_accuracy: 0.9168\n",
      "Epoch 25/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9134 - val_loss: 0.2378 - val_accuracy: 0.9168\n",
      "Epoch 26/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9218 - val_loss: 0.2319 - val_accuracy: 0.9143\n",
      "Epoch 27/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9256 - val_loss: 0.2349 - val_accuracy: 0.9140\n",
      "Epoch 28/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9284 - val_loss: 0.2298 - val_accuracy: 0.9185\n",
      "Epoch 29/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9325 - val_loss: 0.2055 - val_accuracy: 0.9310\n",
      "Epoch 30/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9336 - val_loss: 0.2226 - val_accuracy: 0.9220\n",
      "Epoch 31/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9340 - val_loss: 0.2096 - val_accuracy: 0.9265\n",
      "Epoch 32/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9369 - val_loss: 0.2403 - val_accuracy: 0.9175\n",
      "Epoch 33/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9366 - val_loss: 0.1922 - val_accuracy: 0.9377\n",
      "Epoch 34/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9384 - val_loss: 0.2087 - val_accuracy: 0.9323\n",
      "Epoch 35/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9394 - val_loss: 0.2467 - val_accuracy: 0.9103\n",
      "Epoch 36/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9398 - val_loss: 0.2091 - val_accuracy: 0.9295\n",
      "Epoch 37/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1656 - accuracy: 0.9392 - val_loss: 0.2004 - val_accuracy: 0.9350\n",
      "Epoch 38/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9405 - val_loss: 0.1862 - val_accuracy: 0.9395\n",
      "Epoch 39/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9405 - val_loss: 0.1935 - val_accuracy: 0.9352\n",
      "Epoch 40/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9409 - val_loss: 0.1876 - val_accuracy: 0.9402\n",
      "Epoch 41/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9446 - val_loss: 0.2045 - val_accuracy: 0.9293\n",
      "Epoch 42/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9438 - val_loss: 0.1857 - val_accuracy: 0.9413\n",
      "Epoch 43/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9429 - val_loss: 0.1827 - val_accuracy: 0.9410\n",
      "Epoch 44/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9454 - val_loss: 0.1882 - val_accuracy: 0.9385\n",
      "Epoch 45/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9424 - val_loss: 0.1857 - val_accuracy: 0.9380\n",
      "Epoch 46/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9411 - val_loss: 0.1919 - val_accuracy: 0.9370\n",
      "Epoch 47/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9454 - val_loss: 0.1823 - val_accuracy: 0.9392\n",
      "Epoch 48/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9469 - val_loss: 0.1878 - val_accuracy: 0.9370\n",
      "Epoch 49/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9445 - val_loss: 0.1799 - val_accuracy: 0.9410\n",
      "Epoch 50/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9434 - val_loss: 0.1850 - val_accuracy: 0.9373\n",
      "Epoch 51/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9425 - val_loss: 0.1854 - val_accuracy: 0.9390\n",
      "Epoch 52/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9460 - val_loss: 0.1843 - val_accuracy: 0.9395\n",
      "Epoch 53/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9461 - val_loss: 0.1778 - val_accuracy: 0.9417\n",
      "Epoch 54/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9466 - val_loss: 0.2278 - val_accuracy: 0.9235\n",
      "Epoch 55/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9451 - val_loss: 0.1863 - val_accuracy: 0.9377\n",
      "Epoch 56/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9448 - val_loss: 0.1773 - val_accuracy: 0.9417\n",
      "Epoch 57/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9449 - val_loss: 0.1848 - val_accuracy: 0.9408\n",
      "Epoch 58/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9449 - val_loss: 0.1845 - val_accuracy: 0.9383\n",
      "Epoch 59/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9461 - val_loss: 0.1842 - val_accuracy: 0.9383\n",
      "Epoch 60/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9470 - val_loss: 0.1834 - val_accuracy: 0.9390\n",
      "Epoch 61/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9459 - val_loss: 0.2057 - val_accuracy: 0.9323\n",
      "Epoch 62/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9473 - val_loss: 0.1803 - val_accuracy: 0.9427\n",
      "Epoch 63/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9457 - val_loss: 0.1839 - val_accuracy: 0.9377\n",
      "Epoch 64/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9436 - val_loss: 0.1852 - val_accuracy: 0.9398\n",
      "Epoch 65/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9461 - val_loss: 0.1758 - val_accuracy: 0.9430\n",
      "Epoch 66/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9439 - val_loss: 0.1837 - val_accuracy: 0.9380\n",
      "Epoch 67/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9467 - val_loss: 0.1821 - val_accuracy: 0.9400\n",
      "Epoch 68/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9464 - val_loss: 0.1793 - val_accuracy: 0.9413\n",
      "Epoch 69/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9485 - val_loss: 0.1781 - val_accuracy: 0.9430\n",
      "Epoch 70/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9503 - val_loss: 0.1723 - val_accuracy: 0.9425\n",
      "Epoch 71/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9495 - val_loss: 0.1792 - val_accuracy: 0.9398\n",
      "Epoch 72/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9470 - val_loss: 0.2038 - val_accuracy: 0.9340\n",
      "Epoch 73/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9500 - val_loss: 0.2103 - val_accuracy: 0.9293\n",
      "Epoch 74/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9469 - val_loss: 0.1882 - val_accuracy: 0.9380\n",
      "Epoch 75/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9489 - val_loss: 0.2001 - val_accuracy: 0.9335\n",
      "Epoch 76/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9491 - val_loss: 0.1864 - val_accuracy: 0.9365\n",
      "Epoch 77/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9477 - val_loss: 0.1772 - val_accuracy: 0.9400\n",
      "Epoch 78/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9454 - val_loss: 0.1744 - val_accuracy: 0.9433\n",
      "Epoch 79/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9484 - val_loss: 0.1736 - val_accuracy: 0.9433\n",
      "Epoch 80/200\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9474 - val_loss: 0.1733 - val_accuracy: 0.9427\n"
     ]
    }
   ],
   "source": [
    "lay0=Input(shape=(n_variables,),batch_size=1)\n",
    "lay1=Dense(n_variables+1, activation='relu')(lay0)\n",
    "lay2=Dense(n_variables+1, activation='relu')(lay1)\n",
    "lay3=Dense(n_variables+1, activation='relu')(lay2)\n",
    "lay4=Dense(1,activation='sigmoid')(lay3)\n",
    "output_model=Model(inputs=lay0,outputs=lay4)\n",
    "\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "input=Input(shape=(n_variables,),batch_size=bsize)\n",
    "x=Dense(n_variables+1, activation='relu')(input)\n",
    "x=Dense(n_variables+1, activation='relu')(x)\n",
    "x=Dense(n_variables+1, activation='relu')(x)\n",
    "output=Dense(1,activation='sigmoid')(x)\n",
    "model_ce=Model(inputs=input,outputs=output)\n",
    "model_ce.compile(loss='binary_crossentropy',metrics='accuracy',optimizer=opt)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-5, mode=min, restore_best_weights=True)\n",
    "history_ce = model_ce.fit(x_ce_train, y_ce_train,\n",
    "                          batch_size=bsize,\n",
    "                          epochs=200,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_ce_valid, y_ce_valid),\n",
    "                          callbacks=[early_stop]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb06e-22e5-4ecb-b2c5-48b591268733",
   "metadata": {},
   "source": [
    "## Create and train a Boosted Decision Tree\n",
    "Here, instead of using a MLP, we use a [_Gradient Boosted Decision Tree_](https://xgboost.readthedocs.io/en/stable/) (BDT) to distinguish between signal (true CE hits) and background (fake CE hits). We use the defualt hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "698e5894-b4e3-405c-a99b-c76b2f12403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:15:11] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brownd/mambaforge/envs/KKTrain/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgboost = XGBClassifier()\n",
    "model_xgboost.fit(x_ce_train, y_ce_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa6212-21f7-46c8-b7af-dab07ced4269",
   "metadata": {},
   "source": [
    "Here we can finally apply our two models (the MLP and the BDT) to our test datasets and create the corresponding ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dcbef70-e875-40dd-8a8f-5a66fc32fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 829us/step\n"
     ]
    }
   ],
   "source": [
    "#prediction_ce = model_ce.predict(x_ce_test).ravel()\n",
    "prediction_ce = model_ce.predict(x_ce_test)\n",
    "fpr_ce, tpr_ce, th_ce = roc_curve(y_ce_test,  prediction_ce)\n",
    "auc_ce = roc_auc_score(y_ce_test, prediction_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce6a838-1a4c-41e3-a2bb-46a270ad6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xgboost = model_xgboost.predict_proba(x_ce_test)[:,1]\n",
    "fpr_xgboost, tpr_xgboost, th_xgboost = roc_curve(y_ce_test,  prediction_xgboost)\n",
    "auc_xgboost = roc_auc_score(y_ce_test, prediction_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f392a2-223f-4a47-81de-45dc56b8e42d",
   "metadata": {},
   "source": [
    "The plot of the ROC curves clearly shows that the BDT outperforms the MLP. In principle, however, it should be possible to improve the MLP performances by optimizing the hyperparameters (learning rate, hidden layers, activation functions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def9b80f-d130-4ac0-bc19-ea03de7032f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(tpr_ce,1-fpr_ce,label=f'MLP AUC: {auc_ce:.3f}')\n",
    "ax.plot(tpr_xgboost,1-fpr_xgboost,label=f'BDT AUC: {auc_xgboost:.3f}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Signal efficiency\")\n",
    "ax.set_ylabel(\"Background rejection\")\n",
    "ax.set_xlim(0,1.05)\n",
    "ax.set_ylim(0,1.05)\n",
    "fig.savefig(\"truece.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db6b41-b065-469d-a42d-70a92329ec12",
   "metadata": {},
   "source": [
    "Now we save our model in HDF5 format.  This can be used as input to the SOFIE parser.  Note that the kernel must be restarted when saving this file, as re-running individual cells increments the layer numbers in the hdf5 file, causing the SOFIE parser to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18723eb7-258e-4687-a58c-cd0afa2c41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(1, 9)]                  0         \n",
      "                                                                 \n",
      " dense (Dense)               (1, 10)                   100       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, 10)                   110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (1, 10)                   110       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (1, 1)                    11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "output_model.set_weights(model_ce.get_weights())\n",
    "output_model.summary()\n",
    "output_model.save(\"TrainBkg.h5\")\n",
    "#model_ce.save(\"TrainBkg.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KKTrain",
   "language": "python",
   "name": "kktrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
