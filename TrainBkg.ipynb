{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74a62f-a4a9-4e61-9415-58c8a06f5f75",
   "metadata": {},
   "source": [
    "# Hit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61cb5d-7c43-4a94-9e3c-eb2889e69b88",
   "metadata": {},
   "source": [
    "In this notebook we will try to use Keras and XGBoost in order to distinguish between _true_ conversion electron hits and _fake_ conversion electron hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ce5426-86e5-4f6e-a7d5-a847d6e2ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 14:26:39.849764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import uproot \n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, ReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b18db5-7d46-40ed-9a1b-528d48256097",
   "metadata": {},
   "source": [
    "First of all we use [uproot](https://uproot.readthedocs.io/en/latest/), a library to reading ROOT files in pure Python and NumPy, to open the `trkana` tree in the `TAKK` folder of the `KKSM01.root` file. We only need certain branches, so we apply a filter to read only `de`, `detsh`, `detshmc`, and `demc`. We then apply a mask to our tree to select only good fits.\n",
    "\n",
    "In our problem we have a different number of signal and background entries in our input dataset. There are several techniques avaialable for _unbalanced_ datasets. Here we are using the most naive one, which is just using $\\min(N_{sig}, N_{bkg})$ events. Then, we divide our input into the _training_, _validation_, and _test_ datasets.  Note that the datasets must be pruned to the nearest multiple of the batch size, otherwise the gradient calculation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90450e58-8bce-4fae-be27-97d62d89be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using files in /Users/brownd/data/42082053/files.txt\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000005.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000005.root\n",
      " with 580548 hits\n",
      "[ True  True  True ...  True  True  True]\n",
      "[False False False ... False False False]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000008.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000008.root\n",
      " with 571683 hits\n",
      "[ True  True  True ...  True  True  True]\n",
      "[False False False ... False False False]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000024.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000024.root\n",
      " with 575947 hits\n",
      "[ True  True  True ...  True  True False]\n",
      "[False False False ... False False  True]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000073.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000073.root\n",
      " with 571081 hits\n",
      "[ True  True  True ... False  True  True]\n",
      "[False False False ... False False False]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000103.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000103.root\n",
      " with 572939 hits\n",
      "[ True  True  True ...  True  True  True]\n",
      "[False False False ... False False False]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000126.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000126.root\n",
      " with 578793 hits\n",
      "[ True  True  True ...  True  True  True]\n",
      "[False False False ... False False False]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000173.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000173.root\n",
      " with 568795 hits\n",
      "[ True  True  True ...  True  True False]\n",
      "[False False False ... False False False]\n",
      "Processing: /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000390.root\n",
      "\n",
      "Processed file /Users/brownd/data/42082053/nts.brownd.KKSeed.KKSNB.001210_00000390.root\n",
      " with 572541 hits\n",
      "[ True  True  True ...  True False False]\n",
      "[False False False ... False False False]\n",
      "Total dataset 4592327 hits\n",
      "[ True  True  True ...  True False False]\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "input_dataset = np.empty\n",
    "temp = np.empty\n",
    "signal = np.empty\n",
    "backgnd = np.empty\n",
    "filelist = os.environ['KKTrainData']\n",
    "print(\"Using files in \" + filelist)\n",
    "files = open(filelist, 'r')\n",
    "for filename in files:\n",
    "    print(\"Processing: \" + filename)    \n",
    "    with uproot.open(filename) as file:\n",
    "        trkana = file[\"TAKK\"][\"trkana\"].arrays(filter_name=\"/de|detsh|detshmc|demc/i\")\n",
    "        trkana = trkana[(trkana['de.goodfit']==1)&(trkana['de.status']>0)&(trkana['demc.proc']==167)]\n",
    "        udt = ak.concatenate(trkana['detsh.udt']).to_numpy()\n",
    "        hstate = ak.concatenate(trkana['detsh.state']).to_numpy()\n",
    "        udoca = ak.concatenate(trkana['detsh.udoca']).to_numpy()\n",
    "        rdrift = ak.concatenate(trkana['detsh.rdrift']).to_numpy()\n",
    "        tottdrift = ak.concatenate(trkana['detsh.tottdrift']).to_numpy()\n",
    "        edep = ak.concatenate(trkana['detsh.edep']).to_numpy()\n",
    "        udocavar = ak.concatenate(trkana['detsh.udocavar']).to_numpy()\n",
    "        wdist = ak.concatenate(trkana['detsh.wdist']).to_numpy()\n",
    "        uupos = ak.concatenate(trkana['detsh.uupos']).to_numpy()\n",
    "        rho = np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fX']).to_numpy())\n",
    "        rho = np.add(rho,np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fY']).to_numpy()))\n",
    "        rho = np.sqrt(rho)\n",
    "        print(\"Processed file \" + filename + \" with %s hits\"%udt.shape[0])\n",
    "        temp = np.vstack((udt,udoca,tottdrift,rdrift,edep, udocavar, wdist, uupos, rho)).T\n",
    "        if input_dataset is np.empty:\n",
    "            input_dataset = temp\n",
    "        else:\n",
    "            input_dataset = np.concatenate((input_dataset, temp))\n",
    "        mcrel = []\n",
    "        for i, this_dt in enumerate(trkana['detsh.udt']):\n",
    "            mcrel.extend(trkana['detshmc.rel._rel'][i][:len(this_dt)])\n",
    "        mcrel = np.array(mcrel)\n",
    "        sig = (hstate>-2) & (mcrel==0)\n",
    "        bkg = (hstate>-2) & (mcrel==-1)\n",
    "        if signal is np.empty:\n",
    "            signal = sig\n",
    "            backgnd = bkg\n",
    "        else:\n",
    "            signal = np.concatenate((signal,sig))\n",
    "            backgnd = np.concatenate((backgnd,bkg))\n",
    "nhits=len(input_dataset)\n",
    "print(\"Total dataset %s hits\"%nhits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afa96c-4517-4bca-95c5-53ef01a36dc8",
   "metadata": {},
   "source": [
    "Then, we concatenate each hit variables into a single, large numpy array. These arrays are then stacked in a single bi-dimensional array with `np.vstack`, which will be our input dataset used for the training of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167fc47-dd84-4a31-98d0-a0270f04bfd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we then assign a label to each hit as _signal_ or _background_, depending on the Monte Carlo truth information. Since the dimension of  `detshmc.rel._rel` is not guaranteed to be the same as the dimension of `detsh` we need to loop over all the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed7b23e-d3ab-4645-a905-c3d4090eb7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 25024 matched hits\n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(input_dataset[signal]), len(input_dataset[backgnd]))\n",
    "bsize=32\n",
    "# I need to double the batch_size when truncating as we divide the sample in half later for training\n",
    "tsize=2*bsize\n",
    "min_len = min_len - min_len%tsize\n",
    "print(\"Training on %s matched hits\"%min_len)\n",
    "signal_dataset = input_dataset[signal][:min_len]\n",
    "bkg_dataset = input_dataset[backgnd][:min_len]\n",
    "\n",
    "balanced_input = np.concatenate((signal_dataset, bkg_dataset))\n",
    "y_balanced_input = np.concatenate((np.ones(signal_dataset.shape[0]), np.zeros(bkg_dataset.shape[0])))\n",
    "\n",
    "n_variables = balanced_input.shape[1]\n",
    "\n",
    "x_ce_train, x_ce_test, y_ce_train, y_ce_test = train_test_split(balanced_input, y_balanced_input, test_size=0.5, random_state=42)\n",
    "x_ce_test, x_ce_valid, y_ce_test, y_ce_valid = train_test_split(x_ce_test, y_ce_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414715a1-8fe6-47d2-b856-76e7f09a30c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create and train a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3968ba-51a2-45c8-9882-c989a3f7d691",
   "metadata": {},
   "source": [
    "Here we create a _multi-layer perceptron_ (MLP) model which consists of 3 fully-connected (or _dense_) layers, each one followed by a _dropout_ layer, which helps to avoid overfitting. The model is trained using the [Adam](https://arxiv.org/abs/1412.6980) optimizer and trained for 50 epochs or until the validation loss doesn't improve for 5 epochs (`early_stop`). The model we save must be created first, with an explicit input layer with explicit batch_size, otherwise it can't be parsed by the TMVA::SOFIE parser we use to generate a C++ inference function downstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907660a-6c6a-47cd-a59f-1321824bfb4e",
   "metadata": {},
   "source": [
    "The output model must have batch_size=1, otherwise the SOFIE inference function will assume that many input variables at a time.  Training (gradient calculation) however is much more efficient with a larger batch size, so we construct a separate model for that.  After training and before saving, we'll copy the weights (which don't depend on batch_size) from the trained model to the output model.  This should be unnecessary in the next verison of ROOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d67f9b-a0f8-4b94-8051-c9a5001eed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-23 14:30:25.189083: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 1ms/step - loss: 3.9986 - accuracy: 0.6841 - val_loss: 0.3860 - val_accuracy: 0.8358\n",
      "Epoch 2/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3924 - accuracy: 0.8333 - val_loss: 0.3686 - val_accuracy: 0.8427\n",
      "Epoch 3/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3759 - accuracy: 0.8411 - val_loss: 0.3743 - val_accuracy: 0.8367\n",
      "Epoch 4/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3752 - accuracy: 0.8406 - val_loss: 0.3526 - val_accuracy: 0.8517\n",
      "Epoch 5/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3778 - accuracy: 0.8394 - val_loss: 0.3695 - val_accuracy: 0.8391\n",
      "Epoch 6/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3746 - accuracy: 0.8419 - val_loss: 0.3506 - val_accuracy: 0.8523\n",
      "Epoch 7/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3741 - accuracy: 0.8416 - val_loss: 0.3564 - val_accuracy: 0.8511\n",
      "Epoch 8/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3714 - accuracy: 0.8402 - val_loss: 0.3714 - val_accuracy: 0.8445\n",
      "Epoch 9/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3718 - accuracy: 0.8426 - val_loss: 0.3567 - val_accuracy: 0.8496\n",
      "Epoch 10/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3653 - accuracy: 0.8440 - val_loss: 0.3987 - val_accuracy: 0.8387\n",
      "Epoch 11/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3714 - accuracy: 0.8419 - val_loss: 0.3517 - val_accuracy: 0.8496\n",
      "Epoch 12/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3653 - accuracy: 0.8430 - val_loss: 0.3461 - val_accuracy: 0.8545\n",
      "Epoch 13/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3640 - accuracy: 0.8452 - val_loss: 0.3491 - val_accuracy: 0.8504\n",
      "Epoch 14/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3649 - accuracy: 0.8430 - val_loss: 0.3508 - val_accuracy: 0.8497\n",
      "Epoch 15/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3642 - accuracy: 0.8429 - val_loss: 0.3490 - val_accuracy: 0.8516\n",
      "Epoch 16/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3656 - accuracy: 0.8453 - val_loss: 0.3546 - val_accuracy: 0.8450\n",
      "Epoch 17/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3610 - accuracy: 0.8441 - val_loss: 0.3645 - val_accuracy: 0.8461\n",
      "Epoch 18/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3605 - accuracy: 0.8449 - val_loss: 0.3566 - val_accuracy: 0.8495\n",
      "Epoch 19/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3576 - accuracy: 0.8489 - val_loss: 0.3474 - val_accuracy: 0.8494\n",
      "Epoch 20/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3596 - accuracy: 0.8471 - val_loss: 0.3459 - val_accuracy: 0.8518\n",
      "Epoch 21/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3588 - accuracy: 0.8452 - val_loss: 0.3462 - val_accuracy: 0.8511\n",
      "Epoch 22/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3562 - accuracy: 0.8465 - val_loss: 0.3555 - val_accuracy: 0.8496\n",
      "Epoch 23/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3549 - accuracy: 0.8482 - val_loss: 0.3421 - val_accuracy: 0.8526\n",
      "Epoch 24/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3540 - accuracy: 0.8497 - val_loss: 0.3480 - val_accuracy: 0.8489\n",
      "Epoch 25/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3540 - accuracy: 0.8489 - val_loss: 0.3447 - val_accuracy: 0.8525\n",
      "Epoch 26/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3536 - accuracy: 0.8485 - val_loss: 0.3461 - val_accuracy: 0.8533\n",
      "Epoch 27/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3529 - accuracy: 0.8492 - val_loss: 0.3397 - val_accuracy: 0.8549\n",
      "Epoch 28/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3515 - accuracy: 0.8485 - val_loss: 0.3396 - val_accuracy: 0.8549\n",
      "Epoch 29/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3512 - accuracy: 0.8504 - val_loss: 0.3558 - val_accuracy: 0.8438\n",
      "Epoch 30/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3513 - accuracy: 0.8507 - val_loss: 0.3455 - val_accuracy: 0.8521\n",
      "Epoch 31/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3523 - accuracy: 0.8500 - val_loss: 0.3419 - val_accuracy: 0.8529\n",
      "Epoch 32/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3504 - accuracy: 0.8504 - val_loss: 0.3441 - val_accuracy: 0.8545\n",
      "Epoch 33/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3491 - accuracy: 0.8502 - val_loss: 0.3394 - val_accuracy: 0.8555\n",
      "Epoch 34/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3513 - accuracy: 0.8497 - val_loss: 0.3472 - val_accuracy: 0.8523\n",
      "Epoch 35/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3514 - accuracy: 0.8507 - val_loss: 0.3392 - val_accuracy: 0.8560\n",
      "Epoch 36/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3482 - accuracy: 0.8512 - val_loss: 0.3461 - val_accuracy: 0.8541\n",
      "Epoch 37/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3468 - accuracy: 0.8518 - val_loss: 0.3410 - val_accuracy: 0.8563\n",
      "Epoch 38/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3449 - accuracy: 0.8546 - val_loss: 0.3402 - val_accuracy: 0.8549\n",
      "Epoch 39/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3446 - accuracy: 0.8542 - val_loss: 0.3342 - val_accuracy: 0.8585\n",
      "Epoch 40/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3396 - accuracy: 0.8579 - val_loss: 0.3301 - val_accuracy: 0.8640\n",
      "Epoch 41/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3358 - accuracy: 0.8593 - val_loss: 0.3266 - val_accuracy: 0.8660\n",
      "Epoch 42/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3318 - accuracy: 0.8633 - val_loss: 0.3287 - val_accuracy: 0.8625\n",
      "Epoch 43/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3279 - accuracy: 0.8665 - val_loss: 0.3193 - val_accuracy: 0.8669\n",
      "Epoch 44/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3245 - accuracy: 0.8673 - val_loss: 0.3203 - val_accuracy: 0.8648\n",
      "Epoch 45/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3200 - accuracy: 0.8696 - val_loss: 0.3179 - val_accuracy: 0.8688\n",
      "Epoch 46/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3186 - accuracy: 0.8703 - val_loss: 0.3078 - val_accuracy: 0.8725\n",
      "Epoch 47/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3161 - accuracy: 0.8716 - val_loss: 0.3072 - val_accuracy: 0.8728\n",
      "Epoch 48/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3139 - accuracy: 0.8724 - val_loss: 0.3039 - val_accuracy: 0.8755\n",
      "Epoch 49/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3121 - accuracy: 0.8731 - val_loss: 0.3026 - val_accuracy: 0.8776\n",
      "Epoch 50/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3098 - accuracy: 0.8752 - val_loss: 0.3075 - val_accuracy: 0.8739\n",
      "Epoch 51/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3072 - accuracy: 0.8766 - val_loss: 0.3106 - val_accuracy: 0.8724\n",
      "Epoch 52/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3056 - accuracy: 0.8751 - val_loss: 0.2950 - val_accuracy: 0.8788\n",
      "Epoch 53/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3049 - accuracy: 0.8769 - val_loss: 0.2964 - val_accuracy: 0.8772\n",
      "Epoch 54/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3034 - accuracy: 0.8777 - val_loss: 0.2925 - val_accuracy: 0.8816\n",
      "Epoch 55/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3002 - accuracy: 0.8797 - val_loss: 0.3009 - val_accuracy: 0.8782\n",
      "Epoch 56/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2999 - accuracy: 0.8788 - val_loss: 0.2913 - val_accuracy: 0.8822\n",
      "Epoch 57/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2961 - accuracy: 0.8802 - val_loss: 0.2910 - val_accuracy: 0.8812\n",
      "Epoch 58/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2953 - accuracy: 0.8811 - val_loss: 0.3102 - val_accuracy: 0.8724\n",
      "Epoch 59/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2933 - accuracy: 0.8809 - val_loss: 0.2926 - val_accuracy: 0.8804\n",
      "Epoch 60/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2930 - accuracy: 0.8831 - val_loss: 0.2846 - val_accuracy: 0.8845\n",
      "Epoch 61/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2923 - accuracy: 0.8803 - val_loss: 0.2895 - val_accuracy: 0.8800\n",
      "Epoch 62/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2908 - accuracy: 0.8824 - val_loss: 0.2984 - val_accuracy: 0.8776\n",
      "Epoch 63/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2893 - accuracy: 0.8830 - val_loss: 0.2992 - val_accuracy: 0.8784\n",
      "Epoch 64/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2885 - accuracy: 0.8846 - val_loss: 0.2835 - val_accuracy: 0.8838\n",
      "Epoch 65/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2885 - accuracy: 0.8838 - val_loss: 0.2879 - val_accuracy: 0.8807\n",
      "Epoch 66/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2858 - accuracy: 0.8840 - val_loss: 0.2766 - val_accuracy: 0.8853\n",
      "Epoch 67/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2847 - accuracy: 0.8857 - val_loss: 0.2940 - val_accuracy: 0.8778\n",
      "Epoch 68/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2832 - accuracy: 0.8859 - val_loss: 0.2795 - val_accuracy: 0.8861\n",
      "Epoch 69/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2826 - accuracy: 0.8851 - val_loss: 0.2751 - val_accuracy: 0.8875\n",
      "Epoch 70/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2819 - accuracy: 0.8859 - val_loss: 0.2744 - val_accuracy: 0.8903\n",
      "Epoch 71/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2798 - accuracy: 0.8874 - val_loss: 0.2745 - val_accuracy: 0.8890\n",
      "Epoch 72/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2769 - accuracy: 0.8886 - val_loss: 0.2762 - val_accuracy: 0.8872\n",
      "Epoch 73/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2739 - accuracy: 0.8892 - val_loss: 0.2730 - val_accuracy: 0.8903\n",
      "Epoch 74/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2700 - accuracy: 0.8897 - val_loss: 0.2605 - val_accuracy: 0.8958\n",
      "Epoch 75/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2591 - accuracy: 0.8968 - val_loss: 0.2493 - val_accuracy: 0.9032\n",
      "Epoch 76/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2506 - accuracy: 0.9021 - val_loss: 0.2369 - val_accuracy: 0.9110\n",
      "Epoch 77/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2416 - accuracy: 0.9071 - val_loss: 0.2321 - val_accuracy: 0.9121\n",
      "Epoch 78/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2327 - accuracy: 0.9112 - val_loss: 0.2213 - val_accuracy: 0.9170\n",
      "Epoch 79/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2283 - accuracy: 0.9136 - val_loss: 0.2244 - val_accuracy: 0.9146\n",
      "Epoch 80/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2196 - accuracy: 0.9166 - val_loss: 0.2161 - val_accuracy: 0.9196\n",
      "Epoch 81/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2146 - accuracy: 0.9185 - val_loss: 0.2037 - val_accuracy: 0.9245\n",
      "Epoch 82/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2087 - accuracy: 0.9218 - val_loss: 0.1985 - val_accuracy: 0.9270\n",
      "Epoch 83/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2053 - accuracy: 0.9246 - val_loss: 0.1967 - val_accuracy: 0.9281\n",
      "Epoch 84/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2132 - accuracy: 0.9246 - val_loss: 0.2063 - val_accuracy: 0.9221\n",
      "Epoch 85/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2001 - accuracy: 0.9253 - val_loss: 0.2020 - val_accuracy: 0.9263\n",
      "Epoch 86/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1968 - accuracy: 0.9273 - val_loss: 0.1956 - val_accuracy: 0.9264\n",
      "Epoch 87/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1931 - accuracy: 0.9291 - val_loss: 0.1875 - val_accuracy: 0.9295\n",
      "Epoch 88/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1930 - accuracy: 0.9292 - val_loss: 0.1943 - val_accuracy: 0.9295\n",
      "Epoch 89/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1910 - accuracy: 0.9295 - val_loss: 0.1852 - val_accuracy: 0.9317\n",
      "Epoch 90/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1886 - accuracy: 0.9326 - val_loss: 0.1819 - val_accuracy: 0.9337\n",
      "Epoch 91/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1883 - accuracy: 0.9317 - val_loss: 0.1881 - val_accuracy: 0.9295\n",
      "Epoch 92/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1884 - accuracy: 0.9308 - val_loss: 0.1834 - val_accuracy: 0.9317\n",
      "Epoch 93/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1869 - accuracy: 0.9326 - val_loss: 0.1861 - val_accuracy: 0.9322\n",
      "Epoch 94/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1875 - accuracy: 0.9330 - val_loss: 0.1833 - val_accuracy: 0.9357\n",
      "Epoch 95/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1869 - accuracy: 0.9323 - val_loss: 0.1909 - val_accuracy: 0.9338\n",
      "Epoch 96/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1856 - accuracy: 0.9339 - val_loss: 0.1814 - val_accuracy: 0.9330\n",
      "Epoch 97/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1833 - accuracy: 0.9335 - val_loss: 0.1814 - val_accuracy: 0.9349\n",
      "Epoch 98/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1838 - accuracy: 0.9343 - val_loss: 0.1847 - val_accuracy: 0.9324\n",
      "Epoch 99/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1842 - accuracy: 0.9340 - val_loss: 0.1762 - val_accuracy: 0.9362\n",
      "Epoch 100/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.9339 - val_loss: 0.1744 - val_accuracy: 0.9362\n",
      "Epoch 101/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.9345 - val_loss: 0.1933 - val_accuracy: 0.9297\n",
      "Epoch 102/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.9338 - val_loss: 0.1735 - val_accuracy: 0.9369\n",
      "Epoch 103/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1834 - accuracy: 0.9348 - val_loss: 0.1750 - val_accuracy: 0.9369\n",
      "Epoch 104/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.9356 - val_loss: 0.1726 - val_accuracy: 0.9368\n",
      "Epoch 105/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1814 - accuracy: 0.9347 - val_loss: 0.1760 - val_accuracy: 0.9350\n",
      "Epoch 106/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1797 - accuracy: 0.9349 - val_loss: 0.1749 - val_accuracy: 0.9362\n",
      "Epoch 107/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1794 - accuracy: 0.9351 - val_loss: 0.1733 - val_accuracy: 0.9385\n",
      "Epoch 108/200\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9345 - val_loss: 0.1744 - val_accuracy: 0.9384\n",
      "Epoch 109/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.9357 - val_loss: 0.1745 - val_accuracy: 0.9369\n",
      "Epoch 110/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.9358 - val_loss: 0.1836 - val_accuracy: 0.9321\n",
      "Epoch 111/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.9356 - val_loss: 0.1772 - val_accuracy: 0.9341\n",
      "Epoch 112/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1784 - accuracy: 0.9371 - val_loss: 0.1776 - val_accuracy: 0.9337\n",
      "Epoch 113/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1803 - accuracy: 0.9343 - val_loss: 0.1727 - val_accuracy: 0.9369\n",
      "Epoch 114/200\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.9360 - val_loss: 0.1968 - val_accuracy: 0.9298\n"
     ]
    }
   ],
   "source": [
    "lay0=Input(shape=(n_variables,),batch_size=1)\n",
    "lay1=Dense(n_variables+1, activation='relu')(lay0)\n",
    "lay2=Dense(n_variables+1, activation='relu')(lay1)\n",
    "lay3=Dense(n_variables+1, activation='relu')(lay2)\n",
    "lay4=Dense(1,activation='sigmoid')(lay3)\n",
    "output_model=Model(inputs=lay0,outputs=lay4)\n",
    "\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "input=Input(shape=(n_variables,),batch_size=bsize)\n",
    "x=Dense(n_variables+1, activation='relu')(input)\n",
    "x=Dense(n_variables+1, activation='relu')(x)\n",
    "x=Dense(n_variables+1, activation='relu')(x)\n",
    "output=Dense(1,activation='sigmoid')(x)\n",
    "model_ce=Model(inputs=input,outputs=output)\n",
    "model_ce.compile(loss='binary_crossentropy',metrics='accuracy',optimizer=opt)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-5, restore_best_weights=True)\n",
    "history_ce = model_ce.fit(x_ce_train, y_ce_train,\n",
    "                          batch_size=bsize,\n",
    "                          epochs=200,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_ce_valid, y_ce_valid),\n",
    "                          callbacks=[early_stop]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb06e-22e5-4ecb-b2c5-48b591268733",
   "metadata": {},
   "source": [
    "## Create and train a Boosted Decision Tree\n",
    "Here, instead of using a MLP, we use a [_Gradient Boosted Decision Tree_](https://xgboost.readthedocs.io/en/stable/) (BDT) to distinguish between signal (true CE hits) and background (fake CE hits). We use the defualt hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698e5894-b4e3-405c-a99b-c76b2f12403b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:32:05] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brownd/mambaforge/envs/KKTrain/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgboost = XGBClassifier()\n",
    "model_xgboost.fit(x_ce_train, y_ce_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa6212-21f7-46c8-b7af-dab07ced4269",
   "metadata": {},
   "source": [
    "Here we can finally apply our two models (the MLP and the BDT) to our test datasets and create the corresponding ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcbef70-e875-40dd-8a8f-5a66fc32fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 0s 574us/step\n"
     ]
    }
   ],
   "source": [
    "#prediction_ce = model_ce.predict(x_ce_test).ravel()\n",
    "prediction_ce = model_ce.predict(x_ce_test)\n",
    "fpr_ce, tpr_ce, th_ce = roc_curve(y_ce_test,  prediction_ce)\n",
    "auc_ce = roc_auc_score(y_ce_test, prediction_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce6a838-1a4c-41e3-a2bb-46a270ad6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xgboost = model_xgboost.predict_proba(x_ce_test)[:,1]\n",
    "fpr_xgboost, tpr_xgboost, th_xgboost = roc_curve(y_ce_test,  prediction_xgboost)\n",
    "auc_xgboost = roc_auc_score(y_ce_test, prediction_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f392a2-223f-4a47-81de-45dc56b8e42d",
   "metadata": {},
   "source": [
    "The plot of the ROC curves clearly shows that the BDT outperforms the MLP. In principle, however, it should be possible to improve the MLP performances by optimizing the hyperparameters (learning rate, hidden layers, activation functions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def9b80f-d130-4ac0-bc19-ea03de7032f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(tpr_ce,1-fpr_ce,label=f'MLP AUC: {auc_ce:.3f}')\n",
    "ax.plot(tpr_xgboost,1-fpr_xgboost,label=f'BDT AUC: {auc_xgboost:.3f}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Signal efficiency\")\n",
    "ax.set_ylabel(\"Background rejection\")\n",
    "ax.set_xlim(0,1.05)\n",
    "ax.set_ylim(0,1.05)\n",
    "fig.savefig(\"TrainBkg.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db6b41-b065-469d-a42d-70a92329ec12",
   "metadata": {},
   "source": [
    "Now we save our model in HDF5 format.  This can be used as input to the SOFIE parser.  Note that the kernel must be restarted when saving this file, as re-running individual cells increments the layer numbers in the hdf5 file, causing the SOFIE parser to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18723eb7-258e-4687-a58c-cd0afa2c41e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(1, 9)]                  0         \n",
      "                                                                 \n",
      " dense (Dense)               (1, 10)                   100       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, 10)                   110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (1, 10)                   110       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (1, 1)                    11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "output_model.set_weights(model_ce.get_weights())\n",
    "output_model.summary()\n",
    "output_model.save(\"TrainBkg.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KKTrain",
   "language": "python",
   "name": "kktrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
